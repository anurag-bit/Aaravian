{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Benchmarking Four LLM Strategies: RAG, Agentic, Agentic RAG, Graph RAG\n",
        "\n",
        "Run this notebook end-to-end to build indices, execute the benchmark, and produce plots/results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 0.0.1 Requires-Python >=3.8.1,<3.12; 0.1.0 Requires-Python >=3.8.1,<3.12; 0.1.1 Requires-Python >=3.8.1,<3.12; 0.1.2 Requires-Python >=3.8.1,<3.12; 0.1.3 Requires-Python >=3.8.1,<3.12; 0.1.4 Requires-Python >=3.8.1,<3.12; 0.1.5 Requires-Python >=3.9,<3.13; 0.1.6 Requires-Python >=3.9,<3.13; 0.10.0 Requires-Python >=3.8.1,<3.12; 0.10.1 Requires-Python >=3.8.1,<3.12; 0.10.11 Requires-Python >=3.8.1,<3.12; 0.10.12 Requires-Python >=3.8.1,<3.12; 0.10.3 Requires-Python >=3.8.1,<3.12; 0.10.4 Requires-Python >=3.8.1,<3.12; 0.2.0 Requires-Python >=3.9,<3.13; 0.2.1 Requires-Python >=3.9,<3.13; 0.2.10 Requires-Python >=3.9,<3.13; 0.2.11 Requires-Python >=3.9,<3.13; 0.2.12 Requires-Python >=3.9,<3.13; 0.2.13 Requires-Python >=3.9,<3.13; 0.2.14 Requires-Python >=3.9,<3.13; 0.2.15 Requires-Python >=3.9,<3.13; 0.2.16 Requires-Python >=3.9,<3.13; 0.2.17 Requires-Python >=3.9,<3.13; 0.2.2 Requires-Python >=3.9,<3.13; 0.2.3 Requires-Python >=3.9,<3.13; 0.2.4 Requires-Python >=3.9,<3.13; 0.2.5 Requires-Python >=3.9,<3.13; 0.2.6 Requires-Python >=3.9,<3.13; 0.2.7 Requires-Python >=3.9,<3.13; 0.2.8 Requires-Python >=3.9,<3.13; 0.2.9 Requires-Python >=3.9,<3.13; 0.3.0 Requires-Python >=3.9,<3.13; 0.3.1 Requires-Python >=3.9,<3.13; 0.3.2 Requires-Python >=3.9,<3.13; 0.3.3 Requires-Python >=3.9,<3.13; 0.4.0 Requires-Python >=3.9,<3.13; 0.4.1 Requires-Python >=3.9,<3.13; 0.4.2 Requires-Python >=3.9,<3.13; 0.4.3 Requires-Python >=3.9,<3.13; 0.5.0 Requires-Python >=3.9,<3.13; 0.8.43 Requires-Python >=3.8.1,<3.12; 0.8.43.post1 Requires-Python >=3.8.1,<3.12; 0.8.44 Requires-Python >=3.8.1,<3.12; 0.8.45 Requires-Python >=3.8.1,<3.12; 0.8.45.post1 Requires-Python >=3.8.1,<3.12; 0.8.46 Requires-Python >=3.8.1,<3.12; 0.8.47 Requires-Python >=3.8.1,<3.12; 0.8.48 Requires-Python >=3.8.1,<3.12; 0.8.49 Requires-Python >=3.8.1,<3.12; 0.8.50 Requires-Python >=3.8.1,<3.12; 0.8.51 Requires-Python >=3.8.1,<3.12; 0.8.51.post1 Requires-Python >=3.8.1,<3.12; 0.8.52 Requires-Python >=3.8.1,<3.12; 0.8.53 Requires-Python >=3.8.1,<3.12; 0.8.53.post3 Requires-Python >=3.8.1,<3.12; 0.8.54 Requires-Python >=3.8.1,<3.12; 0.8.55 Requires-Python >=3.8.1,<3.12; 0.8.56 Requires-Python >=3.8.1,<3.12; 0.8.57 Requires-Python >=3.8.1,<3.12; 0.8.58 Requires-Python >=3.8.1,<3.12; 0.8.59 Requires-Python >=3.8.1,<3.12; 0.8.61 Requires-Python >=3.8.1,<3.12; 0.8.62 Requires-Python >=3.8.1,<3.12; 0.8.63.post1 Requires-Python >=3.8.1,<3.12; 0.8.63.post2 Requires-Python >=3.8.1,<3.12; 0.8.64 Requires-Python >=3.8.1,<3.12; 0.8.64.post1 Requires-Python >=3.8.1,<3.12; 0.8.65 Requires-Python >=3.8.1,<3.12; 0.8.66 Requires-Python >=3.8.1,<3.12; 0.8.67 Requires-Python >=3.8.1,<3.12; 0.8.68 Requires-Python >=3.8.1,<3.12; 0.8.69 Requires-Python >=3.8.1,<3.12; 0.8.69.post1 Requires-Python >=3.8.1,<3.12; 0.8.69.post2 Requires-Python >=3.8.1,<3.12; 0.9.0 Requires-Python >=3.8.1,<3.12; 0.9.0.post1 Requires-Python >=3.8.1,<3.12; 0.9.0a1 Requires-Python >=3.8.1,<3.12; 0.9.0a2 Requires-Python >=3.8.1,<3.12; 0.9.0a3 Requires-Python >=3.8.1,<3.12; 0.9.1 Requires-Python >=3.8.1,<3.12; 0.9.10 Requires-Python >=3.8.1,<3.12; 0.9.10a1 Requires-Python >=3.8.1,<3.12; 0.9.10a2 Requires-Python >=3.8.1,<3.12; 0.9.11 Requires-Python >=3.8.1,<3.12; 0.9.11.post1 Requires-Python >=3.8.1,<3.12; 0.9.2 Requires-Python >=3.8.1,<3.12; 0.9.3 Requires-Python >=3.8.1,<3.12; 0.9.3.post1 Requires-Python >=3.8.1,<3.12; 0.9.4 Requires-Python >=3.8.1,<3.12; 0.9.5 Requires-Python >=3.8.1,<3.12; 0.9.6 Requires-Python >=3.8.1,<3.12; 0.9.6.post1 Requires-Python >=3.8.1,<3.12; 0.9.6.post2 Requires-Python >=3.8.1,<3.12; 0.9.7 Requires-Python >=3.8.1,<3.12; 0.9.8 Requires-Python >=3.8.1,<3.12; 0.9.8.post1 Requires-Python >=3.8.1,<3.12; 0.9.9 Requires-Python >=3.8.1,<3.12; 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11; 1.26.0 Requires-Python >=3.9,<3.13; 1.26.1 Requires-Python >=3.9,<3.13\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement llama-index-vector-stores-qdrant==0.2.10 (from versions: 0.1.3, 0.1.4, 0.6.0, 0.6.1, 0.7.0, 0.7.1, 0.8.0, 0.8.1, 0.8.2, 0.8.3, 0.8.4, 0.8.5, 0.8.6)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for llama-index-vector-stores-qdrant==0.2.10\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install -q -r ../requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import uuid\n",
        "import yaml\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Determinism\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load config\n",
        "CONFIG_PATH = Path('../configs/config.yaml').resolve()\n",
        "with open(CONFIG_PATH, 'r') as f:\n",
        "    CONFIG = yaml.safe_load(f)\n",
        "\n",
        "ARTIFACTS = CONFIG['artifacts']\n",
        "DATASET = CONFIG['dataset']\n",
        "MODELS = CONFIG['models']\n",
        "PRICING = CONFIG['pricing_usd']\n",
        "\n",
        "# Ensure artifact dirs\n",
        "os.makedirs(Path('..')/Path(ARTIFACTS['plots_dir']), exist_ok=True)\n",
        "os.makedirs(Path('..')/Path(ARTIFACTS['traces_dir']), exist_ok=True)\n",
        "os.makedirs(Path('..')/Path(Path(ARTIFACTS['results_parquet']).parent), exist_ok=True)\n",
        "\n",
        "RUN_ID = str(uuid.uuid4())\n",
        "print('Config loaded:', CONFIG_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset loader\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "def load_golden_dataset(path: str) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    with open(path, 'r') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                rows.append(json.loads(line))\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "GOLDEN_PATH = Path('..')/DATASET['golden_path']\n",
        "DOCS_META_PATH = Path('..')/DATASET['docs_metadata']\n",
        "\n",
        "golden_df = load_golden_dataset(str(GOLDEN_PATH))\n",
        "print('Golden dataset size:', len(golden_df))\n",
        "print(golden_df.head(2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LlamaIndex base settings, reranker, and synthesis helper\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms.openai import OpenAI as OpenAILLM\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
        "\n",
        "Settings.llm = OpenAILLM(model=MODELS['llm']['model'], temperature=MODELS['llm']['temperature'])\n",
        "Settings.embed_model = OpenAIEmbedding(model=MODELS['embeddings']['model'])\n",
        "Settings.text_splitter = SentenceSplitter(chunk_size=CONFIG['retrieval']['chunking']['chunk_tokens'],\n",
        "                                          chunk_overlap=CONFIG['retrieval']['chunking']['overlap_tokens'])\n",
        "\n",
        "cohere_api_key = os.getenv('COHERE_API_KEY')\n",
        "reranker = CohereRerank(api_key=cohere_api_key, top_n=CONFIG['retrieval']['rerank_top_k']) if cohere_api_key else None\n",
        "\n",
        "\n",
        "def synthesize_with_citations(query: str, nodes):\n",
        "    # For simplicity, use index query engine; for stricter grounding, constrain to nodes via custom retriever\n",
        "    query_engine = index.as_query_engine(similarity_top_k=CONFIG['retrieval']['rerank_top_k'])\n",
        "    response = query_engine.query(query)\n",
        "    citations = []\n",
        "    for sn in response.source_nodes:\n",
        "        meta = sn.node.metadata or {}\n",
        "        citations.append({\n",
        "            'doc_id': meta.get('file_name', 'unknown'),\n",
        "            'page': meta.get('page_label', None),\n",
        "        })\n",
        "    return str(response), citations, response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agentic and Agentic-RAG (deterministic minimal versions)\n",
        "import re as _re\n",
        "\n",
        "def calculator_tool(expression: str) -> str:\n",
        "    try:\n",
        "        if not _re.fullmatch(r\"[0-9+\\-*/().\\s]+\", expression):\n",
        "            return \"Unsupported expression\"\n",
        "        return str(eval(expression))\n",
        "    except Exception:\n",
        "        return \"Error\"\n",
        "\n",
        "\n",
        "def build_agent_pipeline(config: dict):\n",
        "    def run(query_obj: dict) -> dict:\n",
        "        q = query_obj['query']\n",
        "        t0 = time.time()\n",
        "        tool_trace = []\n",
        "        nodes = hybrid_retrieve(q)\n",
        "        tool_trace.append({\"tool\":\"retriever\",\"num_nodes\": len(nodes)})\n",
        "        calc_res = None\n",
        "        if _re.search(r\"[0-9]+\\s*[+\\-*/]\", q):\n",
        "            calc_res = calculator_tool(q)\n",
        "            tool_trace.append({\"tool\":\"calculator\",\"result\": calc_res})\n",
        "        t1 = time.time()\n",
        "        answer_text, citations, raw_resp = synthesize_with_citations(q, nodes)\n",
        "        if calc_res and calc_res not in (\"Error\",\"Unsupported expression\"):\n",
        "            answer_text = f\"{answer_text}\\nComputed value: {calc_res}\"\n",
        "        llm_ms = int((time.time()-t1)*1000)\n",
        "        total_ms = int((time.time()-t0)*1000)\n",
        "        tokens_prompt = max(1, len(q)//3 + sum(len(n.get_text()) for n in nodes)//4)\n",
        "        tokens_completion = max(1, len(answer_text)//3)\n",
        "        return {\n",
        "            'answer_text': answer_text,\n",
        "            'used_context': [n.get_text() for n in nodes[:CONFIG['retrieval']['rerank_top_k']]],\n",
        "            'citations': citations,\n",
        "            'timings': { 'total_ms': total_ms, 'tools_ms': total_ms-llm_ms, 'llm_ms': llm_ms },\n",
        "            'tokens': { 'prompt': tokens_prompt, 'completion': tokens_completion },\n",
        "            'cost': {},\n",
        "            'trace': { 'strategy': 'AGENT', 'tool_trace': tool_trace },\n",
        "            'errors': []\n",
        "        }\n",
        "    return run\n",
        "\n",
        "\n",
        "def build_agentic_rag_pipeline(config: dict):\n",
        "    threshold = 0.4\n",
        "    def simple_verifier(answer: str, nodes) -> float:\n",
        "        ctx = \" \".join(n.get_text() for n in nodes)\n",
        "        overlap = sum(1 for w in answer.split() if w in ctx)\n",
        "        return overlap / max(1, len(answer.split()))\n",
        "    def run(query_obj: dict) -> dict:\n",
        "        q = query_obj['query']\n",
        "        t0 = time.time()\n",
        "        tool_trace = []\n",
        "        nodes = hybrid_retrieve(q)\n",
        "        tool_trace.append({\"tool\":\"retriever\",\"num_nodes\": len(nodes)})\n",
        "        if reranker:\n",
        "            nodes = reranker.postprocess_nodes(nodes)[:CONFIG['retrieval']['rerank_top_k']]\n",
        "            tool_trace.append({\"tool\":\"reranker\",\"kept\": len(nodes)})\n",
        "        t1 = time.time()\n",
        "        answer_text, citations, raw_resp = synthesize_with_citations(q, nodes)\n",
        "        score = simple_verifier(answer_text, nodes)\n",
        "        if score < threshold:\n",
        "            nodes2 = hybrid_retrieve(q)\n",
        "            answer_text, citations, raw_resp = synthesize_with_citations(q, nodes2)\n",
        "            tool_trace.append({\"tool\":\"verifier\",\"score\": score, \"action\":\"retry\"})\n",
        "        llm_ms = int((time.time()-t1)*1000)\n",
        "        total_ms = int((time.time()-t0)*1000)\n",
        "        tokens_prompt = max(1, len(q)//3 + sum(len(n.get_text()) for n in nodes)//4)\n",
        "        tokens_completion = max(1, len(answer_text)//3)\n",
        "        return {\n",
        "            'answer_text': answer_text,\n",
        "            'used_context': [n.get_text() for n in nodes[:CONFIG['retrieval']['rerank_top_k']]],\n",
        "            'citations': citations,\n",
        "            'timings': { 'total_ms': total_ms, 'tools_ms': total_ms-llm_ms, 'llm_ms': llm_ms },\n",
        "            'tokens': { 'prompt': tokens_prompt, 'completion': tokens_completion },\n",
        "            'cost': {},\n",
        "            'trace': { 'strategy': 'AGENTIC_RAG', 'tool_trace': tool_trace },\n",
        "            'errors': []\n",
        "        }\n",
        "    return run\n",
        "\n",
        "# Register/ensure pipelines dict exists\n",
        "try:\n",
        "    pipelines\n",
        "except NameError:\n",
        "    pipelines = {}\n",
        "\n",
        "pipelines['AGENT'] = build_agent_pipeline(CONFIG)\n",
        "pipelines['AGENTIC_RAG'] = build_agentic_rag_pipeline(CONFIG)\n",
        "print('Agentic and Agentic-RAG ready.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Qdrant persistent vector store + hybrid retrieval\n",
        "from qdrant_client import QdrantClient\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "from llama_index.core import StorageContext, VectorStoreIndex, SimpleDirectoryReader\n",
        "try:\n",
        "    from llama_index.retrievers.bm25 import BM25Retriever\n",
        "except Exception:\n",
        "    BM25Retriever = None\n",
        "\n",
        "VS = CONFIG['vector_store']\n",
        "HYB = CONFIG['hybrid']\n",
        "\n",
        "qdrant_client = QdrantClient(host=VS['host'], port=VS['port'])\n",
        "if VS['recreate']:\n",
        "    try:\n",
        "        qdrant_client.delete_collection(collection_name=VS['collection'])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# Build index if not exists\n",
        "storage_context = StorageContext.from_defaults(\n",
        "    vector_store=QdrantVectorStore(client=qdrant_client, collection_name=VS['collection'])\n",
        ")\n",
        "\n",
        "# Rebuild documents if index empty; reuse earlier loaded documents if present\n",
        "if 'documents' not in globals() or len(documents) == 0:\n",
        "    DOCS_DIR = Path('..')/ 'data' / 'docs'\n",
        "    reader = SimpleDirectoryReader(str(DOCS_DIR), recursive=True, required_exts=['.txt'])\n",
        "    documents = reader.load_data()\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
        "retriever_dense = index.as_retriever(similarity_top_k=CONFIG['retrieval']['dense_top_k'])\n",
        "retriever_bm25 = BM25Retriever.from_defaults(documents=documents, similarity_top_k=HYB['bm25_top_k']) if (HYB['enable_bm25'] and BM25Retriever) else None\n",
        "\n",
        "print('Qdrant collection ready:', VS['collection'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace RAG pipeline to use hybrid retrieval and numeric/table boost\n",
        "import re\n",
        "\n",
        "NUM_RE = re.compile(CONFIG['tables']['number_regex'])\n",
        "NUMERIC_BOOST = CONFIG['tables']['numeric_chunk_boost']\n",
        "\n",
        "\n",
        "def hybrid_retrieve(query: str):\n",
        "    nodes_dense = retriever_dense.retrieve(query)\n",
        "    if retriever_bm25:\n",
        "        nodes_bm25 = retriever_bm25.retrieve(query)\n",
        "    else:\n",
        "        nodes_bm25 = []\n",
        "    # simple merge by node id with score sum; small numeric boost if query has numbers\n",
        "    has_numbers = bool(NUM_RE.search(query))\n",
        "    merged = {}\n",
        "    for n in nodes_dense:\n",
        "        merged[n.node.node_id] = {\"node\": n, \"score\": getattr(n, 'score', 1.0)}\n",
        "    for n in nodes_bm25:\n",
        "        if n.node.node_id in merged:\n",
        "            merged[n.node.node_id][\"score\"] += getattr(n, 'score', 1.0)\n",
        "        else:\n",
        "            merged[n.node.node_id] = {\"node\": n, \"score\": getattr(n, 'score', 1.0)}\n",
        "    items = list(merged.values())\n",
        "    if has_numbers:\n",
        "        for it in items:\n",
        "            if NUM_RE.search(it[\"node\"].get_text()):\n",
        "                it[\"score\"] *= NUMERIC_BOOST\n",
        "    items.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "    return [it[\"node\"] for it in items]\n",
        "\n",
        "# Re-define build_rag_pipeline to use hybrid\n",
        "\n",
        "def build_rag_pipeline(config: dict):\n",
        "    def run(query_obj: dict) -> dict:\n",
        "        q = query_obj['query']\n",
        "        t0 = time.time()\n",
        "        nodes = hybrid_retrieve(q)\n",
        "        rt_ms = int((time.time()-t0)*1000)\n",
        "        if reranker:\n",
        "            nodes = reranker.postprocess_nodes(nodes)[:config['retrieval']['rerank_top_k']]\n",
        "        t1 = time.time()\n",
        "        answer_text, citations, raw_resp = synthesize_with_citations(q, nodes)\n",
        "        llm_ms = int((time.time()-t1)*1000)\n",
        "        total_ms = int((time.time()-t0)*1000)\n",
        "        tokens_prompt = max(1, len(q)//3 + sum(len(n.get_text()) for n in nodes)//4)\n",
        "        tokens_completion = max(1, len(answer_text)//3)\n",
        "        return {\n",
        "            'answer_text': answer_text,\n",
        "            'used_context': [n.get_text() for n in nodes[:config['retrieval']['rerank_top_k']]],\n",
        "            'citations': citations,\n",
        "            'timings': { 'total_ms': total_ms, 'retrieval_ms': rt_ms, 'llm_ms': llm_ms },\n",
        "            'tokens': { 'prompt': tokens_prompt, 'completion': tokens_completion },\n",
        "            'cost': {},\n",
        "            'trace': { 'strategy': 'RAG' },\n",
        "            'errors': []\n",
        "        }\n",
        "    return run\n",
        "\n",
        "pipelines['RAG'] = build_rag_pipeline(CONFIG)\n",
        "print('RAG updated to hybrid + Qdrant persistence')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LLM-as-Judge with rubric + citation/numeric checks\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "with open(Path('..')/MODELS['judge']['rubric_prompt_path'], 'r') as f:\n",
        "    JUDGE_RUBRIC = f.read()\n",
        "\n",
        "NUM_RE = re.compile(CONFIG['tables']['number_regex'])\n",
        "\n",
        "\n",
        "def normalize_numbers(text: str):\n",
        "    nums = NUM_RE.findall(text or '')\n",
        "    # normalize by removing commas and $ symbol\n",
        "    return set(n.replace(',', '').replace('$','') for n in nums)\n",
        "\n",
        "\n",
        "def citation_correct(pred_citations: list, gt_citations: list) -> bool:\n",
        "    if not gt_citations:\n",
        "        return True\n",
        "    pred_docs = {c.get('doc_id') for c in (pred_citations or [])}\n",
        "    gt_docs = {c.get('doc_id') for c in (gt_citations or [])}\n",
        "    overlap = len(pred_docs & gt_docs)\n",
        "    return overlap >= max(1, len(gt_docs) // 2)\n",
        "\n",
        "\n",
        "def llm_judge(question: str, answer: str, contexts: list, ground_truth: str, ground_citations: list) -> dict:\n",
        "    sys = JUDGE_RUBRIC\n",
        "    ctx = \"\\n---\\n\".join(contexts[:5])\n",
        "    content = f\"Question: {question}\\nAnswer: {answer}\\nProvided_context:\\n{ctx}\\nGround_truth: {ground_truth}\\n\"\n",
        "    resp = client.chat.completions.create(\n",
        "        model=MODELS['judge']['model'],\n",
        "        temperature=MODELS['judge']['temperature'],\n",
        "        messages=[\n",
        "            {\"role\":\"system\",\"content\": sys},\n",
        "            {\"role\":\"user\",\"content\": content}\n",
        "        ]\n",
        "    )\n",
        "    text = resp.choices[0].message.content\n",
        "    try:\n",
        "        data = json.loads(text)\n",
        "    except Exception:\n",
        "        # fallback heuristic\n",
        "        data = {\"faithfulness\": 0.5, \"answer_relevancy\": 0.5, \"citation_correct\": False, \"numeric_exact\": False, \"notes\": \"parse_fail\"}\n",
        "    # numeric exact additional check\n",
        "    pred_nums = normalize_numbers(answer)\n",
        "    gt_nums = normalize_numbers(ground_truth)\n",
        "    numeric_exact = bool(pred_nums & gt_nums) or data.get('numeric_exact', False)\n",
        "    # citation check overriding with structural\n",
        "    citation_ok = citation_correct([], ground_citations) if 'citations' not in data else data.get('citation_correct', False)\n",
        "    return {\n",
        "        'faithfulness_score': float(data.get('faithfulness', 0.0)),\n",
        "        'answer_relevancy': float(data.get('answer_relevancy', 0.0)),\n",
        "        'context_precision': None,\n",
        "        'context_recall': None,\n",
        "        'citation_exact': bool(citation_ok),\n",
        "        'numeric_exact': bool(numeric_exact)\n",
        "    }\n",
        "\n",
        "# Fallback judge stub\n",
        "\n",
        "def judge_result(result_dict: dict, ground_truth: dict, config: dict) -> dict:\n",
        "    return {\n",
        "        'faithfulness_score': 0.5,\n",
        "        'answer_relevancy': 0.5,\n",
        "        'context_precision': None,\n",
        "        'context_recall': None,\n",
        "        'citation_exact': False,\n",
        "        'numeric_exact': False\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Integrate judge into evaluation loop and improve token/cost accounting\n",
        "import tiktoken\n",
        "\n",
        "enc_in = tiktoken.get_encoding('cl100k_base')\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    try:\n",
        "        return len(enc_in.encode(text or ''))\n",
        "    except Exception:\n",
        "        return max(1, len(text or '')//3)\n",
        "\n",
        "\n",
        "def estimate_cost_usd_tokens(prompt_toks: int, completion_toks: int, model_key: str) -> tuple:\n",
        "    pricing = PRICING['openai'][model_key]\n",
        "    in_cost = (prompt_toks/1_000_000) * pricing['input_per_million']\n",
        "    out_cost = (completion_toks/1_000_000) * pricing['output_per_million']\n",
        "    return in_cost + out_cost, in_cost, out_cost\n",
        "\n",
        "# Replace judge_result call site by LLM judge where possible\n",
        "new_rows = []\n",
        "for i, row in golden_df.iterrows():\n",
        "    query_obj = row.to_dict()\n",
        "    for strategy_name, runner in pipelines.items():\n",
        "        r = runner(query_obj)\n",
        "        # Judge\n",
        "        try:\n",
        "            scores = llm_judge(\n",
        "                question=row['query'],\n",
        "                answer=r['answer_text'],\n",
        "                contexts=r['used_context'],\n",
        "                ground_truth=row.get('ground_truth_answer',''),\n",
        "                ground_citations=row.get('ground_truth_citations', [])\n",
        "            )\n",
        "        except Exception:\n",
        "            scores = judge_result(r, query_obj, CONFIG)\n",
        "        # Tokens/costs\n",
        "        p_tok = count_tokens(row['query']) + sum(count_tokens(c) for c in r['used_context'])\n",
        "        c_tok = count_tokens(r['answer_text'])\n",
        "        total_cost, in_cost, out_cost = estimate_cost_usd_tokens(p_tok, c_tok, MODELS['llm']['model'])\n",
        "        record = {\n",
        "            'query_id': row['query_id'],\n",
        "            'job_story': row['job_story'],\n",
        "            'difficulty': row.get('difficulty',''),\n",
        "            'strategy': strategy_name,\n",
        "            'run_id': RUN_ID,\n",
        "            'answer_text': r['answer_text'],\n",
        "            'answer_tokens': c_tok,\n",
        "            'context_chunk_ids': [],\n",
        "            'faithfulness_score': scores['faithfulness_score'],\n",
        "            'answer_relevancy': scores['answer_relevancy'],\n",
        "            'context_precision': scores.get('context_precision'),\n",
        "            'context_recall': scores.get('context_recall'),\n",
        "            'citation_exact': scores['citation_exact'],\n",
        "            'numeric_exact': scores['numeric_exact'],\n",
        "            'hallucination_flag': False,\n",
        "            'refusal_flag': False,\n",
        "            'latency_ms_total': r['timings'].get('total_ms'),\n",
        "            'latency_ms_retrieval': r['timings'].get('retrieval_ms'),\n",
        "            'latency_ms_llm': r['timings'].get('llm_ms'),\n",
        "            'latency_ms_tools': r['timings'].get('tools_ms'),\n",
        "            'first_token_ms': None,\n",
        "            'tokens_prompt': p_tok,\n",
        "            'tokens_completion': c_tok,\n",
        "            'tokens_total': p_tok + c_tok,\n",
        "            'cost_usd_total': total_cost,\n",
        "            'cost_usd_llm': total_cost,\n",
        "            'cost_usd_embeddings': 0.0,\n",
        "            'cost_usd_rerank': 0.0,\n",
        "            'cost_usd_tools': 0.0,\n",
        "            'ingestion_time_s': None,\n",
        "            'ingestion_cost_usd': None,\n",
        "            'errors': r['errors'],\n",
        "            'trace': r['trace'],\n",
        "        }\n",
        "        new_rows.append(record)\n",
        "\n",
        "results_df = pd.DataFrame(new_rows)\n",
        "print('Evaluation with LLM judge complete. Rows:', len(results_df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Minimal Neo4j Graph RAG connector with graceful fallback\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "NEO = CONFIG['neo4j']\n",
        "neo4j_driver = None\n",
        "try:\n",
        "    neo4j_driver = GraphDatabase.driver(NEO['uri'], auth=(NEO['user'], os.getenv(NEO['password_env'])))\n",
        "    with neo4j_driver.session(database=NEO['database']) as session:\n",
        "        if NEO['enforce_constraints']:\n",
        "            session.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (e:Entity) REQUIRE e.id IS UNIQUE\")\n",
        "    print('Neo4j connected')\n",
        "except Exception as e:\n",
        "    print('Neo4j not available, Graph RAG will use stub. Reason:', e)\n",
        "\n",
        "\n",
        "def graph_traverse(query: str):\n",
        "    if not neo4j_driver:\n",
        "        return []\n",
        "    with neo4j_driver.session(database=NEO['database']) as session:\n",
        "        # Very simple traversal by keyword\n",
        "        res = session.run(\"MATCH (e:Entity) WHERE toLower(e.name) CONTAINS toLower($q) RETURN e.name as name LIMIT 20\", q=query)\n",
        "        return [r['name'] for r in res]\n",
        "\n",
        "# Replace Graph RAG run() to use graph traversal hits as additional context when available\n",
        "\n",
        "def build_graph_rag_pipeline(config: dict):\n",
        "    def run(query_obj: dict) -> dict:\n",
        "        q = query_obj['query']\n",
        "        t0 = time.time()\n",
        "        kg_hits = graph_traverse(q)\n",
        "        nodes = hybrid_retrieve(q)\n",
        "        if reranker:\n",
        "            nodes = reranker.postprocess_nodes(nodes)[:config['retrieval']['rerank_top_k']]\n",
        "        t1 = time.time()\n",
        "        answer_text, citations, raw_resp = synthesize_with_citations(q, nodes)\n",
        "        llm_ms = int((time.time()-t1)*1000)\n",
        "        total_ms = int((time.time()-t0)*1000)\n",
        "        tokens_prompt = max(1, len(q)//3 + sum(len(n.get_text()) for n in nodes)//4)\n",
        "        tokens_completion = max(1, len(answer_text)//3)\n",
        "        return {\n",
        "            'answer_text': answer_text,\n",
        "            'used_context': [n.get_text() for n in nodes[:config['retrieval']['rerank_top_k']]],\n",
        "            'citations': citations,\n",
        "            'timings': { 'total_ms': total_ms, 'graph_ms': 5 if kg_hits else 0, 'retrieval_ms': total_ms-llm_ms-5, 'llm_ms': llm_ms },\n",
        "            'tokens': { 'prompt': tokens_prompt, 'completion': tokens_completion },\n",
        "            'cost': {},\n",
        "            'trace': { 'strategy': 'GRAPH_RAG', 'kg_trace': kg_hits[:10] },\n",
        "            'errors': []\n",
        "        }\n",
        "    return run\n",
        "\n",
        "pipelines['GRAPH_RAG'] = build_graph_rag_pipeline(CONFIG)\n",
        "print('Graph RAG wired to Neo4j (with fallback).')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analytics and plots (Plotly only to avoid native builds)\n",
        "import plotly.express as px\n",
        "\n",
        "agg = results_df.groupby(['strategy']).agg({\n",
        "    'faithfulness_score':'mean',\n",
        "    'answer_relevancy':'mean',\n",
        "    'latency_ms_total':'median',\n",
        "    'cost_usd_total':'mean'\n",
        "}).reset_index()\n",
        "\n",
        "fig1 = px.bar(agg, x='strategy', y='faithfulness_score', title='Average Faithfulness by Strategy')\n",
        "fig1.write_image('../plots/avg_faithfulness_by_strategy.png')\n",
        "fig1.show()\n",
        "\n",
        "fig2 = px.box(results_df, x='strategy', y='latency_ms_total', title='Latency Distribution by Strategy')\n",
        "fig2.write_image('../plots/latency_distribution_by_strategy.png')\n",
        "fig2.show()\n",
        "\n",
        "fig3 = px.scatter(agg, x='cost_usd_total', y='answer_relevancy', color='strategy', size_max=18, title='Cost vs Answer Relevancy')\n",
        "fig3.write_image('../plots/cost_vs_answer_relevancy.png')\n",
        "fig3.show()\n",
        "\n",
        "(Path('..')/ 'results').mkdir(exist_ok=True)\n",
        "agg.to_csv('../results/report-ready.csv', index=False)\n",
        "results_df.to_parquet('../results/results.parquet', index=False)\n",
        "print('Saved artifacts to ../results and ../plots')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
