{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Benchmarking Four LLM Strategies: RAG, Agentic, Agentic RAG, Graph RAG\n",
        "\n",
        "Run this notebook end-to-end to build indices, execute the benchmark, and produce plots/results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m \u001b[31m[94 lines of output]\u001b[0m\n",
            "  \u001b[31m   \u001b[0m \u001b[36m\u001b[1m+ meson setup /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493 /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/.mesonpy-warq0neo/build -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --vsenv --native-file=/tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/.mesonpy-warq0neo/build/meson-python-native-file.ini\u001b[0m\n",
            "  \u001b[31m   \u001b[0m The Meson build system\n",
            "  \u001b[31m   \u001b[0m Version: 1.2.1\n",
            "  \u001b[31m   \u001b[0m Source dir: /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493\n",
            "  \u001b[31m   \u001b[0m Build dir: /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/.mesonpy-warq0neo/build\n",
            "  \u001b[31m   \u001b[0m Build type: native build\n",
            "  \u001b[31m   \u001b[0m Project name: pandas\n",
            "  \u001b[31m   \u001b[0m Project version: 2.2.2\n",
            "  \u001b[31m   \u001b[0m C compiler for the host machine: cc (gcc 15.2.0 \"cc (Ubuntu 15.2.0-4ubuntu4) 15.2.0\")\n",
            "  \u001b[31m   \u001b[0m C linker for the host machine: cc ld.bfd 2.45\n",
            "  \u001b[31m   \u001b[0m C++ compiler for the host machine: c++ (gcc 15.2.0 \"c++ (Ubuntu 15.2.0-4ubuntu4) 15.2.0\")\n",
            "  \u001b[31m   \u001b[0m C++ linker for the host machine: c++ ld.bfd 2.45\n",
            "  \u001b[31m   \u001b[0m Cython compiler for the host machine: cython (cython 3.0.5)\n",
            "  \u001b[31m   \u001b[0m Host machine cpu family: x86_64\n",
            "  \u001b[31m   \u001b[0m Host machine cpu: x86_64\n",
            "  \u001b[31m   \u001b[0m Program python found: YES (/home/anuragisinsane/projects/Aaravian/.venv/bin/python)\n",
            "  \u001b[31m   \u001b[0m Did not find pkg-config by name 'pkg-config'\n",
            "  \u001b[31m   \u001b[0m Found Pkg-config: NO\n",
            "  \u001b[31m   \u001b[0m Run-time dependency python found: YES 3.13\n",
            "  \u001b[31m   \u001b[0m Build targets in project: 53\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m pandas 2.2.2\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m   User defined options\n",
            "  \u001b[31m   \u001b[0m     Native files: /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/.mesonpy-warq0neo/build/meson-python-native-file.ini\n",
            "  \u001b[31m   \u001b[0m     buildtype   : release\n",
            "  \u001b[31m   \u001b[0m     vsenv       : True\n",
            "  \u001b[31m   \u001b[0m     b_ndebug    : if-release\n",
            "  \u001b[31m   \u001b[0m     b_vscrt     : md\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m Found ninja-1.13.0.git.kitware.jobserver-pipe-1 at /tmp/pip-build-env-0h8csi6e/normal/bin/ninja\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m Visual Studio environment is needed to run Ninja. It is recommended to use Meson wrapper:\n",
            "  \u001b[31m   \u001b[0m /tmp/pip-build-env-0h8csi6e/overlay/bin/meson compile -C .\n",
            "  \u001b[31m   \u001b[0m \u001b[36m\u001b[1m+ /tmp/pip-build-env-0h8csi6e/normal/bin/ninja\u001b[0m\n",
            "  \u001b[31m   \u001b[0m [1/151] Generating pandas/_libs/khash_primitive_helper_pxi with a custom command\n",
            "  \u001b[31m   \u001b[0m [2/151] Generating pandas/_libs/algos_take_helper_pxi with a custom command\n",
            "  \u001b[31m   \u001b[0m [3/151] Generating pandas/_libs/index_class_helper_pxi with a custom command\n",
            "  \u001b[31m   \u001b[0m [4/151] Generating pandas/_libs/hashtable_class_helper_pxi with a custom command\n",
            "  \u001b[31m   \u001b[0m [5/151] Generating pandas/_libs/hashtable_func_helper_pxi with a custom command\n",
            "  \u001b[31m   \u001b[0m [6/151] Generating pandas/_libs/sparse_op_helper_pxi with a custom command\n",
            "  \u001b[31m   \u001b[0m [7/151] Generating pandas/_libs/algos_common_helper_pxi with a custom command\n",
            "  \u001b[31m   \u001b[0m [8/151] Generating pandas/_libs/intervaltree_helper_pxi with a custom command\n",
            "  \u001b[31m   \u001b[0m [9/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/tslibs/base.pyx\n",
            "  \u001b[31m   \u001b[0m [10/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/tslibs/nattype.pyx\n",
            "  \u001b[31m   \u001b[0m warning: /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/tslibs/nattype.pyx:79:0: Global name __nat_unpickle matched from within class scope in contradiction to to Python 'class private name' rules. This may change in a future release.\n",
            "  \u001b[31m   \u001b[0m warning: /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/tslibs/nattype.pyx:79:0: Global name __nat_unpickle matched from within class scope in contradiction to to Python 'class private name' rules. This may change in a future release.\n",
            "  \u001b[31m   \u001b[0m [11/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/tslibs/ccalendar.pyx\n",
            "  \u001b[31m   \u001b[0m [12/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/tslibs/np_datetime.pyx\n",
            "  \u001b[31m   \u001b[0m [13/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/tslibs/dtypes.pyx\n",
            "  \u001b[31m   \u001b[0m [14/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/tslibs/period.pyx\n",
            "  \u001b[31m   \u001b[0m [15/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/tslibs/conversion.pyx\n",
            "  \u001b[31m   \u001b[0m [16/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/tslibs/timestamps.pyx\n",
            "  \u001b[31m   \u001b[0m [17/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/tslibs/fields.pyx\n",
            "  \u001b[31m   \u001b[0m [18/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/tslibs/parsing.pyx\n",
            "  \u001b[31m   \u001b[0m [19/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/tslibs/timezones.pyx\n",
            "  \u001b[31m   \u001b[0m [20/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/tslibs/strptime.pyx\n",
            "  \u001b[31m   \u001b[0m [21/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/tslibs/offsets.pyx\n",
            "  \u001b[31m   \u001b[0m [22/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/tslibs/tzconversion.pyx\n",
            "  \u001b[31m   \u001b[0m [23/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/arrays.pyx\n",
            "  \u001b[31m   \u001b[0m [24/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/indexing.pyx\n",
            "  \u001b[31m   \u001b[0m [25/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/tslibs/timedeltas.pyx\n",
            "  \u001b[31m   \u001b[0m [26/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/tslibs/vectorized.pyx\n",
            "  \u001b[31m   \u001b[0m [27/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/hashing.pyx\n",
            "  \u001b[31m   \u001b[0m [28/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/internals.pyx\n",
            "  \u001b[31m   \u001b[0m [29/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/missing.pyx\n",
            "  \u001b[31m   \u001b[0m [30/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/index.pyx\n",
            "  \u001b[31m   \u001b[0m [31/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/lib.pyx\n",
            "  \u001b[31m   \u001b[0m [32/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/ops_dispatch.pyx\n",
            "  \u001b[31m   \u001b[0m [33/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/properties.pyx\n",
            "  \u001b[31m   \u001b[0m [34/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/parsers.pyx\n",
            "  \u001b[31m   \u001b[0m [35/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/ops.pyx\n",
            "  \u001b[31m   \u001b[0m [36/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/byteswap.pyx\n",
            "  \u001b[31m   \u001b[0m [37/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/sas.pyx\n",
            "  \u001b[31m   \u001b[0m [38/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/reshape.pyx\n",
            "  \u001b[31m   \u001b[0m [39/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/interval.pyx\n",
            "  \u001b[31m   \u001b[0m [40/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/testing.pyx\n",
            "  \u001b[31m   \u001b[0m [41/151] Compiling C object pandas/_libs/tslibs/base.cpython-313-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_base.pyx.c.o\n",
            "  \u001b[31m   \u001b[0m \u001b[31mFAILED: [code=1] \u001b[0mpandas/_libs/tslibs/base.cpython-313-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_base.pyx.c.o\n",
            "  \u001b[31m   \u001b[0m cc -Ipandas/_libs/tslibs/base.cpython-313-x86_64-linux-gnu.so.p -Ipandas/_libs/tslibs -I../../pandas/_libs/tslibs -I../../../../pip-build-env-0h8csi6e/overlay/lib/python3.13/site-packages/numpy/_core/include -I../../pandas/_libs/include -I/usr/include/python3.13 -fvisibility=hidden -fdiagnostics-color=always -DNDEBUG -D_FILE_OFFSET_BITS=64 -w -std=c11 -O3 -DNPY_NO_DEPRECATED_API=0 -DNPY_TARGET_VERSION=NPY_1_21_API_VERSION -fPIC -MD -MQ pandas/_libs/tslibs/base.cpython-313-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_base.pyx.c.o -MF pandas/_libs/tslibs/base.cpython-313-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_base.pyx.c.o.d -o pandas/_libs/tslibs/base.cpython-313-x86_64-linux-gnu.so.p/meson-generated_pandas__libs_tslibs_base.pyx.c.o -c pandas/_libs/tslibs/base.cpython-313-x86_64-linux-gnu.so.p/pandas/_libs/tslibs/base.pyx.c\n",
            "  \u001b[31m   \u001b[0m \u001b[01m\u001b[Kpandas/_libs/tslibs/base.cpython-313-x86_64-linux-gnu.so.p/pandas/_libs/tslibs/base.pyx.c:16:10:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[KPython.h: No such file or directory\n",
            "  \u001b[31m   \u001b[0m    16 | #include \u001b[01;31m\u001b[K\"Python.h\"\u001b[m\u001b[K\n",
            "  \u001b[31m   \u001b[0m       |          \u001b[01;31m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "  \u001b[31m   \u001b[0m compilation terminated.\n",
            "  \u001b[31m   \u001b[0m [42/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/groupby.pyx\n",
            "  \u001b[31m   \u001b[0m [43/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/tslib.pyx\n",
            "  \u001b[31m   \u001b[0m [44/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/join.pyx\n",
            "  \u001b[31m   \u001b[0m [45/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/algos.pyx\n",
            "  \u001b[31m   \u001b[0m [46/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/window/indexers.pyx\n",
            "  \u001b[31m   \u001b[0m [47/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/writers.pyx\n",
            "  \u001b[31m   \u001b[0m [48/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/hashtable.pyx\n",
            "  \u001b[31m   \u001b[0m [49/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/window/aggregations.pyx\n",
            "  \u001b[31m   \u001b[0m [50/151] Compiling Cython source /tmp/pip-install-crevujgo/pandas_f9e53a561c1844b986e8de0e2cb09493/pandas/_libs/sparse.pyx\n",
            "  \u001b[31m   \u001b[0m ninja: build stopped: subcommand failed.\n",
            "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m pandas\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install -q -r ../requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import uuid\n",
        "import yaml\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Determinism\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load config\n",
        "CONFIG_PATH = Path('../configs/config.yaml').resolve()\n",
        "with open(CONFIG_PATH, 'r') as f:\n",
        "    CONFIG = yaml.safe_load(f)\n",
        "\n",
        "ARTIFACTS = CONFIG['artifacts']\n",
        "DATASET = CONFIG['dataset']\n",
        "MODELS = CONFIG['models']\n",
        "PRICING = CONFIG['pricing_usd']\n",
        "\n",
        "# Ensure artifact dirs\n",
        "os.makedirs(Path('..')/Path(ARTIFACTS['plots_dir']), exist_ok=True)\n",
        "os.makedirs(Path('..')/Path(ARTIFACTS['traces_dir']), exist_ok=True)\n",
        "os.makedirs(Path('..')/Path(Path(ARTIFACTS['results_parquet']).parent), exist_ok=True)\n",
        "\n",
        "RUN_ID = str(uuid.uuid4())\n",
        "print('Config loaded:', CONFIG_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset loader\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "def load_golden_dataset(path: str) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    with open(path, 'r') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                rows.append(json.loads(line))\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "GOLDEN_PATH = Path('..')/DATASET['golden_path']\n",
        "DOCS_META_PATH = Path('..')/DATASET['docs_metadata']\n",
        "\n",
        "golden_df = load_golden_dataset(str(GOLDEN_PATH))\n",
        "print('Golden dataset size:', len(golden_df))\n",
        "print(golden_df.head(2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LlamaIndex base settings, reranker, and synthesis helper\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms.openai import OpenAI as OpenAILLM\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
        "\n",
        "Settings.llm = OpenAILLM(model=MODELS['llm']['model'], temperature=MODELS['llm']['temperature'])\n",
        "Settings.embed_model = OpenAIEmbedding(model=MODELS['embeddings']['model'])\n",
        "Settings.text_splitter = SentenceSplitter(chunk_size=CONFIG['retrieval']['chunking']['chunk_tokens'],\n",
        "                                          chunk_overlap=CONFIG['retrieval']['chunking']['overlap_tokens'])\n",
        "\n",
        "cohere_api_key = os.getenv('COHERE_API_KEY')\n",
        "reranker = CohereRerank(api_key=cohere_api_key, top_n=CONFIG['retrieval']['rerank_top_k']) if cohere_api_key else None\n",
        "\n",
        "\n",
        "def synthesize_with_citations(query: str, nodes):\n",
        "    # For simplicity, use index query engine; for stricter grounding, constrain to nodes via custom retriever\n",
        "    query_engine = index.as_query_engine(similarity_top_k=CONFIG['retrieval']['rerank_top_k'])\n",
        "    response = query_engine.query(query)\n",
        "    citations = []\n",
        "    for sn in response.source_nodes:\n",
        "        meta = sn.node.metadata or {}\n",
        "        citations.append({\n",
        "            'doc_id': meta.get('file_name', 'unknown'),\n",
        "            'page': meta.get('page_label', None),\n",
        "        })\n",
        "    return str(response), citations, response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agentic and Agentic-RAG (deterministic minimal versions)\n",
        "import re as _re\n",
        "\n",
        "def calculator_tool(expression: str) -> str:\n",
        "    try:\n",
        "        if not _re.fullmatch(r\"[0-9+\\-*/().\\s]+\", expression):\n",
        "            return \"Unsupported expression\"\n",
        "        return str(eval(expression))\n",
        "    except Exception:\n",
        "        return \"Error\"\n",
        "\n",
        "\n",
        "def build_agent_pipeline(config: dict):\n",
        "    def run(query_obj: dict) -> dict:\n",
        "        q = query_obj['query']\n",
        "        t0 = time.time()\n",
        "        tool_trace = []\n",
        "        nodes = hybrid_retrieve(q)\n",
        "        tool_trace.append({\"tool\":\"retriever\",\"num_nodes\": len(nodes)})\n",
        "        calc_res = None\n",
        "        if _re.search(r\"[0-9]+\\s*[+\\-*/]\", q):\n",
        "            calc_res = calculator_tool(q)\n",
        "            tool_trace.append({\"tool\":\"calculator\",\"result\": calc_res})\n",
        "        t1 = time.time()\n",
        "        answer_text, citations, raw_resp = synthesize_with_citations(q, nodes)\n",
        "        if calc_res and calc_res not in (\"Error\",\"Unsupported expression\"):\n",
        "            answer_text = f\"{answer_text}\\nComputed value: {calc_res}\"\n",
        "        llm_ms = int((time.time()-t1)*1000)\n",
        "        total_ms = int((time.time()-t0)*1000)\n",
        "        tokens_prompt = max(1, len(q)//3 + sum(len(n.get_text()) for n in nodes)//4)\n",
        "        tokens_completion = max(1, len(answer_text)//3)\n",
        "        return {\n",
        "            'answer_text': answer_text,\n",
        "            'used_context': [n.get_text() for n in nodes[:CONFIG['retrieval']['rerank_top_k']]],\n",
        "            'citations': citations,\n",
        "            'timings': { 'total_ms': total_ms, 'tools_ms': total_ms-llm_ms, 'llm_ms': llm_ms },\n",
        "            'tokens': { 'prompt': tokens_prompt, 'completion': tokens_completion },\n",
        "            'cost': {},\n",
        "            'trace': { 'strategy': 'AGENT', 'tool_trace': tool_trace },\n",
        "            'errors': []\n",
        "        }\n",
        "    return run\n",
        "\n",
        "\n",
        "def build_agentic_rag_pipeline(config: dict):\n",
        "    threshold = 0.4\n",
        "    def simple_verifier(answer: str, nodes) -> float:\n",
        "        ctx = \" \".join(n.get_text() for n in nodes)\n",
        "        overlap = sum(1 for w in answer.split() if w in ctx)\n",
        "        return overlap / max(1, len(answer.split()))\n",
        "    def run(query_obj: dict) -> dict:\n",
        "        q = query_obj['query']\n",
        "        t0 = time.time()\n",
        "        tool_trace = []\n",
        "        nodes = hybrid_retrieve(q)\n",
        "        tool_trace.append({\"tool\":\"retriever\",\"num_nodes\": len(nodes)})\n",
        "        if reranker:\n",
        "            nodes = reranker.postprocess_nodes(nodes)[:CONFIG['retrieval']['rerank_top_k']]\n",
        "            tool_trace.append({\"tool\":\"reranker\",\"kept\": len(nodes)})\n",
        "        t1 = time.time()\n",
        "        answer_text, citations, raw_resp = synthesize_with_citations(q, nodes)\n",
        "        score = simple_verifier(answer_text, nodes)\n",
        "        if score < threshold:\n",
        "            nodes2 = hybrid_retrieve(q)\n",
        "            answer_text, citations, raw_resp = synthesize_with_citations(q, nodes2)\n",
        "            tool_trace.append({\"tool\":\"verifier\",\"score\": score, \"action\":\"retry\"})\n",
        "        llm_ms = int((time.time()-t1)*1000)\n",
        "        total_ms = int((time.time()-t0)*1000)\n",
        "        tokens_prompt = max(1, len(q)//3 + sum(len(n.get_text()) for n in nodes)//4)\n",
        "        tokens_completion = max(1, len(answer_text)//3)\n",
        "        return {\n",
        "            'answer_text': answer_text,\n",
        "            'used_context': [n.get_text() for n in nodes[:CONFIG['retrieval']['rerank_top_k']]],\n",
        "            'citations': citations,\n",
        "            'timings': { 'total_ms': total_ms, 'tools_ms': total_ms-llm_ms, 'llm_ms': llm_ms },\n",
        "            'tokens': { 'prompt': tokens_prompt, 'completion': tokens_completion },\n",
        "            'cost': {},\n",
        "            'trace': { 'strategy': 'AGENTIC_RAG', 'tool_trace': tool_trace },\n",
        "            'errors': []\n",
        "        }\n",
        "    return run\n",
        "\n",
        "# Register/ensure pipelines dict exists\n",
        "try:\n",
        "    pipelines\n",
        "except NameError:\n",
        "    pipelines = {}\n",
        "\n",
        "pipelines['AGENT'] = build_agent_pipeline(CONFIG)\n",
        "pipelines['AGENTIC_RAG'] = build_agentic_rag_pipeline(CONFIG)\n",
        "print('Agentic and Agentic-RAG ready.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Qdrant persistent vector store + hybrid retrieval\n",
        "from qdrant_client import QdrantClient\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "from llama_index.core import StorageContext, VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.retrievers.bm25 import BM25Retriever\n",
        "\n",
        "VS = CONFIG['vector_store']\n",
        "HYB = CONFIG['hybrid']\n",
        "\n",
        "qdrant_client = QdrantClient(host=VS['host'], port=VS['port'])\n",
        "if VS['recreate']:\n",
        "    try:\n",
        "        qdrant_client.delete_collection(collection_name=VS['collection'])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# Build index if not exists\n",
        "storage_context = StorageContext.from_defaults(\n",
        "    vector_store=QdrantVectorStore(client=qdrant_client, collection_name=VS['collection'])\n",
        ")\n",
        "\n",
        "# Rebuild documents if index empty; reuse earlier loaded documents if present\n",
        "if 'documents' not in globals() or len(documents) == 0:\n",
        "    DOCS_DIR = Path('..')/ 'data' / 'docs'\n",
        "    reader = SimpleDirectoryReader(str(DOCS_DIR), recursive=True, required_exts=['.txt'])\n",
        "    documents = reader.load_data()\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
        "retriever_dense = index.as_retriever(similarity_top_k=CONFIG['retrieval']['dense_top_k'])\n",
        "retriever_bm25 = BM25Retriever.from_defaults(documents=documents, similarity_top_k=HYB['bm25_top_k']) if HYB['enable_bm25'] else None\n",
        "\n",
        "print('Qdrant collection ready:', VS['collection'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace RAG pipeline to use hybrid retrieval and numeric/table boost\n",
        "import re\n",
        "\n",
        "NUM_RE = re.compile(CONFIG['tables']['number_regex'])\n",
        "NUMERIC_BOOST = CONFIG['tables']['numeric_chunk_boost']\n",
        "\n",
        "\n",
        "def hybrid_retrieve(query: str):\n",
        "    nodes_dense = retriever_dense.retrieve(query)\n",
        "    if retriever_bm25:\n",
        "        nodes_bm25 = retriever_bm25.retrieve(query)\n",
        "    else:\n",
        "        nodes_bm25 = []\n",
        "    # simple merge by node id with score sum; small numeric boost if query has numbers\n",
        "    has_numbers = bool(NUM_RE.search(query))\n",
        "    merged = {}\n",
        "    for n in nodes_dense:\n",
        "        merged[n.node.node_id] = {\"node\": n, \"score\": getattr(n, 'score', 1.0)}\n",
        "    for n in nodes_bm25:\n",
        "        if n.node.node_id in merged:\n",
        "            merged[n.node.node_id][\"score\"] += getattr(n, 'score', 1.0)\n",
        "        else:\n",
        "            merged[n.node.node_id] = {\"node\": n, \"score\": getattr(n, 'score', 1.0)}\n",
        "    items = list(merged.values())\n",
        "    if has_numbers:\n",
        "        for it in items:\n",
        "            if NUM_RE.search(it[\"node\"].get_text()):\n",
        "                it[\"score\"] *= NUMERIC_BOOST\n",
        "    items.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "    return [it[\"node\"] for it in items]\n",
        "\n",
        "# Re-define build_rag_pipeline to use hybrid\n",
        "\n",
        "def build_rag_pipeline(config: dict):\n",
        "    def run(query_obj: dict) -> dict:\n",
        "        q = query_obj['query']\n",
        "        t0 = time.time()\n",
        "        nodes = hybrid_retrieve(q)\n",
        "        rt_ms = int((time.time()-t0)*1000)\n",
        "        if reranker:\n",
        "            nodes = reranker.postprocess_nodes(nodes)[:config['retrieval']['rerank_top_k']]\n",
        "        t1 = time.time()\n",
        "        answer_text, citations, raw_resp = synthesize_with_citations(q, nodes)\n",
        "        llm_ms = int((time.time()-t1)*1000)\n",
        "        total_ms = int((time.time()-t0)*1000)\n",
        "        tokens_prompt = max(1, len(q)//3 + sum(len(n.get_text()) for n in nodes)//4)\n",
        "        tokens_completion = max(1, len(answer_text)//3)\n",
        "        return {\n",
        "            'answer_text': answer_text,\n",
        "            'used_context': [n.get_text() for n in nodes[:config['retrieval']['rerank_top_k']]],\n",
        "            'citations': citations,\n",
        "            'timings': { 'total_ms': total_ms, 'retrieval_ms': rt_ms, 'llm_ms': llm_ms },\n",
        "            'tokens': { 'prompt': tokens_prompt, 'completion': tokens_completion },\n",
        "            'cost': {},\n",
        "            'trace': { 'strategy': 'RAG' },\n",
        "            'errors': []\n",
        "        }\n",
        "    return run\n",
        "\n",
        "pipelines['RAG'] = build_rag_pipeline(CONFIG)\n",
        "print('RAG updated to hybrid + Qdrant persistence')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LLM-as-Judge with rubric + citation/numeric checks\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "with open(Path('..')/MODELS['judge']['rubric_prompt_path'], 'r') as f:\n",
        "    JUDGE_RUBRIC = f.read()\n",
        "\n",
        "NUM_RE = re.compile(CONFIG['tables']['number_regex'])\n",
        "\n",
        "\n",
        "def normalize_numbers(text: str):\n",
        "    nums = NUM_RE.findall(text or '')\n",
        "    # normalize by removing commas and $ symbol\n",
        "    return set(n.replace(',', '').replace('$','') for n in nums)\n",
        "\n",
        "\n",
        "def citation_correct(pred_citations: list, gt_citations: list) -> bool:\n",
        "    if not gt_citations:\n",
        "        return True\n",
        "    pred_docs = {c.get('doc_id') for c in (pred_citations or [])}\n",
        "    gt_docs = {c.get('doc_id') for c in (gt_citations or [])}\n",
        "    overlap = len(pred_docs & gt_docs)\n",
        "    return overlap >= max(1, len(gt_docs) // 2)\n",
        "\n",
        "\n",
        "def llm_judge(question: str, answer: str, contexts: list, ground_truth: str, ground_citations: list) -> dict:\n",
        "    sys = JUDGE_RUBRIC\n",
        "    ctx = \"\\n---\\n\".join(contexts[:5])\n",
        "    content = f\"Question: {question}\\nAnswer: {answer}\\nProvided_context:\\n{ctx}\\nGround_truth: {ground_truth}\\n\"\n",
        "    resp = client.chat.completions.create(\n",
        "        model=MODELS['judge']['model'],\n",
        "        temperature=MODELS['judge']['temperature'],\n",
        "        messages=[\n",
        "            {\"role\":\"system\",\"content\": sys},\n",
        "            {\"role\":\"user\",\"content\": content}\n",
        "        ]\n",
        "    )\n",
        "    text = resp.choices[0].message.content\n",
        "    try:\n",
        "        data = json.loads(text)\n",
        "    except Exception:\n",
        "        # fallback heuristic\n",
        "        data = {\"faithfulness\": 0.5, \"answer_relevancy\": 0.5, \"citation_correct\": False, \"numeric_exact\": False, \"notes\": \"parse_fail\"}\n",
        "    # numeric exact additional check\n",
        "    pred_nums = normalize_numbers(answer)\n",
        "    gt_nums = normalize_numbers(ground_truth)\n",
        "    numeric_exact = bool(pred_nums & gt_nums) or data.get('numeric_exact', False)\n",
        "    # citation check overriding with structural\n",
        "    citation_ok = citation_correct([], ground_citations) if 'citations' not in data else data.get('citation_correct', False)\n",
        "    return {\n",
        "        'faithfulness_score': float(data.get('faithfulness', 0.0)),\n",
        "        'answer_relevancy': float(data.get('answer_relevancy', 0.0)),\n",
        "        'context_precision': None,\n",
        "        'context_recall': None,\n",
        "        'citation_exact': bool(citation_ok),\n",
        "        'numeric_exact': bool(numeric_exact)\n",
        "    }\n",
        "\n",
        "# Fallback judge stub\n",
        "\n",
        "def judge_result(result_dict: dict, ground_truth: dict, config: dict) -> dict:\n",
        "    return {\n",
        "        'faithfulness_score': 0.5,\n",
        "        'answer_relevancy': 0.5,\n",
        "        'context_precision': None,\n",
        "        'context_recall': None,\n",
        "        'citation_exact': False,\n",
        "        'numeric_exact': False\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Integrate judge into evaluation loop and improve token/cost accounting\n",
        "import tiktoken\n",
        "\n",
        "enc_in = tiktoken.get_encoding('cl100k_base')\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    try:\n",
        "        return len(enc_in.encode(text or ''))\n",
        "    except Exception:\n",
        "        return max(1, len(text or '')//3)\n",
        "\n",
        "\n",
        "def estimate_cost_usd_tokens(prompt_toks: int, completion_toks: int, model_key: str) -> tuple:\n",
        "    pricing = PRICING['openai'][model_key]\n",
        "    in_cost = (prompt_toks/1_000_000) * pricing['input_per_million']\n",
        "    out_cost = (completion_toks/1_000_000) * pricing['output_per_million']\n",
        "    return in_cost + out_cost, in_cost, out_cost\n",
        "\n",
        "# Replace judge_result call site by LLM judge where possible\n",
        "new_rows = []\n",
        "for i, row in golden_df.iterrows():\n",
        "    query_obj = row.to_dict()\n",
        "    for strategy_name, runner in pipelines.items():\n",
        "        r = runner(query_obj)\n",
        "        # Judge\n",
        "        try:\n",
        "            scores = llm_judge(\n",
        "                question=row['query'],\n",
        "                answer=r['answer_text'],\n",
        "                contexts=r['used_context'],\n",
        "                ground_truth=row.get('ground_truth_answer',''),\n",
        "                ground_citations=row.get('ground_truth_citations', [])\n",
        "            )\n",
        "        except Exception:\n",
        "            scores = judge_result(r, query_obj, CONFIG)\n",
        "        # Tokens/costs\n",
        "        p_tok = count_tokens(row['query']) + sum(count_tokens(c) for c in r['used_context'])\n",
        "        c_tok = count_tokens(r['answer_text'])\n",
        "        total_cost, in_cost, out_cost = estimate_cost_usd_tokens(p_tok, c_tok, MODELS['llm']['model'])\n",
        "        record = {\n",
        "            'query_id': row['query_id'],\n",
        "            'job_story': row['job_story'],\n",
        "            'difficulty': row.get('difficulty',''),\n",
        "            'strategy': strategy_name,\n",
        "            'run_id': RUN_ID,\n",
        "            'answer_text': r['answer_text'],\n",
        "            'answer_tokens': c_tok,\n",
        "            'context_chunk_ids': [],\n",
        "            'faithfulness_score': scores['faithfulness_score'],\n",
        "            'answer_relevancy': scores['answer_relevancy'],\n",
        "            'context_precision': scores.get('context_precision'),\n",
        "            'context_recall': scores.get('context_recall'),\n",
        "            'citation_exact': scores['citation_exact'],\n",
        "            'numeric_exact': scores['numeric_exact'],\n",
        "            'hallucination_flag': False,\n",
        "            'refusal_flag': False,\n",
        "            'latency_ms_total': r['timings'].get('total_ms'),\n",
        "            'latency_ms_retrieval': r['timings'].get('retrieval_ms'),\n",
        "            'latency_ms_llm': r['timings'].get('llm_ms'),\n",
        "            'latency_ms_tools': r['timings'].get('tools_ms'),\n",
        "            'first_token_ms': None,\n",
        "            'tokens_prompt': p_tok,\n",
        "            'tokens_completion': c_tok,\n",
        "            'tokens_total': p_tok + c_tok,\n",
        "            'cost_usd_total': total_cost,\n",
        "            'cost_usd_llm': total_cost,\n",
        "            'cost_usd_embeddings': 0.0,\n",
        "            'cost_usd_rerank': 0.0,\n",
        "            'cost_usd_tools': 0.0,\n",
        "            'ingestion_time_s': None,\n",
        "            'ingestion_cost_usd': None,\n",
        "            'errors': r['errors'],\n",
        "            'trace': r['trace'],\n",
        "        }\n",
        "        new_rows.append(record)\n",
        "\n",
        "results_df = pd.DataFrame(new_rows)\n",
        "print('Evaluation with LLM judge complete. Rows:', len(results_df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Minimal Neo4j Graph RAG connector with graceful fallback\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "NEO = CONFIG['neo4j']\n",
        "neo4j_driver = None\n",
        "try:\n",
        "    neo4j_driver = GraphDatabase.driver(NEO['uri'], auth=(NEO['user'], os.getenv(NEO['password_env'])))\n",
        "    with neo4j_driver.session(database=NEO['database']) as session:\n",
        "        if NEO['enforce_constraints']:\n",
        "            session.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (e:Entity) REQUIRE e.id IS UNIQUE\")\n",
        "    print('Neo4j connected')\n",
        "except Exception as e:\n",
        "    print('Neo4j not available, Graph RAG will use stub. Reason:', e)\n",
        "\n",
        "\n",
        "def graph_traverse(query: str):\n",
        "    if not neo4j_driver:\n",
        "        return []\n",
        "    with neo4j_driver.session(database=NEO['database']) as session:\n",
        "        # Very simple traversal by keyword\n",
        "        res = session.run(\"MATCH (e:Entity) WHERE toLower(e.name) CONTAINS toLower($q) RETURN e.name as name LIMIT 20\", q=query)\n",
        "        return [r['name'] for r in res]\n",
        "\n",
        "# Replace Graph RAG run() to use graph traversal hits as additional context when available\n",
        "\n",
        "def build_graph_rag_pipeline(config: dict):\n",
        "    def run(query_obj: dict) -> dict:\n",
        "        q = query_obj['query']\n",
        "        t0 = time.time()\n",
        "        kg_hits = graph_traverse(q)\n",
        "        nodes = hybrid_retrieve(q)\n",
        "        if reranker:\n",
        "            nodes = reranker.postprocess_nodes(nodes)[:config['retrieval']['rerank_top_k']]\n",
        "        t1 = time.time()\n",
        "        answer_text, citations, raw_resp = synthesize_with_citations(q, nodes)\n",
        "        llm_ms = int((time.time()-t1)*1000)\n",
        "        total_ms = int((time.time()-t0)*1000)\n",
        "        tokens_prompt = max(1, len(q)//3 + sum(len(n.get_text()) for n in nodes)//4)\n",
        "        tokens_completion = max(1, len(answer_text)//3)\n",
        "        return {\n",
        "            'answer_text': answer_text,\n",
        "            'used_context': [n.get_text() for n in nodes[:config['retrieval']['rerank_top_k']]],\n",
        "            'citations': citations,\n",
        "            'timings': { 'total_ms': total_ms, 'graph_ms': 5 if kg_hits else 0, 'retrieval_ms': total_ms-llm_ms-5, 'llm_ms': llm_ms },\n",
        "            'tokens': { 'prompt': tokens_prompt, 'completion': tokens_completion },\n",
        "            'cost': {},\n",
        "            'trace': { 'strategy': 'GRAPH_RAG', 'kg_trace': kg_hits[:10] },\n",
        "            'errors': []\n",
        "        }\n",
        "    return run\n",
        "\n",
        "pipelines['GRAPH_RAG'] = build_graph_rag_pipeline(CONFIG)\n",
        "print('Graph RAG wired to Neo4j (with fallback).')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analytics and plots\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "agg = results_df.groupby(['strategy']).agg({\n",
        "    'faithfulness_score':'mean',\n",
        "    'answer_relevancy':'mean',\n",
        "    'latency_ms_total':'median',\n",
        "    'cost_usd_total':'mean'\n",
        "}).reset_index()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.barplot(data=agg, x='strategy', y='faithfulness_score')\n",
        "plt.title('Average Faithfulness by Strategy')\n",
        "plt.tight_layout()\n",
        "plt.savefig('../plots/avg_faithfulness_by_strategy.png')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.boxplot(data=results_df, x='strategy', y='latency_ms_total')\n",
        "plt.title('Latency Distribution by Strategy')\n",
        "plt.tight_layout()\n",
        "plt.savefig('../plots/latency_distribution_by_strategy.png')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.scatterplot(data=agg, x='cost_usd_total', y='answer_relevancy', hue='strategy', s=120)\n",
        "plt.title('Cost vs Answer Relevancy')\n",
        "plt.tight_layout()\n",
        "plt.savefig('../plots/cost_vs_answer_relevancy.png')\n",
        "plt.show()\n",
        "\n",
        "(Path('..')/ 'results').mkdir(exist_ok=True)\n",
        "agg.to_csv('../results/report-ready.csv', index=False)\n",
        "results_df.to_parquet('../results/results.parquet', index=False)\n",
        "print('Saved artifacts to ../results and ../plots')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
